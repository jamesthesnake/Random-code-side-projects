#@markdown #**üß† Check Resources**
import multiprocessing
import torch
from psutil import virtual_memory
!nvidia-smi -L
gpu_ram = round(torch.cuda.get_device_properties(0).total_memory / 2**30, 2)
print(f'device: {torch.device("cuda:0" if torch.cuda.is_available() else "cpu").type} ~ vRAM: {gpu_ram} GB ~ CPU: {multiprocessing.cpu_count()} cores ~ CPU-RAM: {round(virtual_memory().total / 1024**3, 1)} GB ~ PyTorch-version: {torch.__version__} ~ CUDA version: {torch.version.cuda} ~ cuDNN version: {torch.backends.cudnn.version()}')
# !nvidia-smi
#@markdown #**üìò Install ruDALL-E and Setup** (~3min)
#@markdown Turn it up to 11 (unrestricted), if you have `High-RAM` Google Colab access.

max_vram = 6  #@param {type:"slider", min:3.5, max:11.0, step:0.5}
ALLOWED_MEMORY = max_vram
dwt_mode = "True = 512x512 (artifacts)"  #@param ["False = 256x256 high-quality", "True = 512x512>256x256", "True = 512x512 (artifacts)"]
#@markdown Turn dwt True for lower RAM to avoid tensor error.

from IPython.display import Javascript
display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 170})'''))  #limit output height
print('install is in progress...')

# !pip install rudalle==0.0.1rc6 > /dev/null
# !pip install rudalle==0.0.1rc7 > /dev/null
# !pip install rudalle==0.0.1rc8 > /dev/null
!pip install rudalle==0.0.1rc10 > /dev/null
# !pip3 install git+https://github.com/sberbank-ai/ru-dalle.git@master

import transformers
import more_itertools
from tqdm.auto import tqdm
from rudalle.pipelines import show, cherry_pick_by_clip
from rudalle import get_rudalle_model, get_tokenizer, get_vae, get_ruclip
from rudalle.utils import seed_everything, torch_tensors_to_pil_list
import multiprocessing
import torch
from psutil import virtual_memory

device = 'cuda'
dalle = get_rudalle_model('Malevich', pretrained=True, fp16=True, device=device)
tokenizer = get_tokenizer()
total_memory = torch.cuda.get_device_properties(0).total_memory / 2**30

if ALLOWED_MEMORY < 10.5: # set low-ram workflow
    DALLE_BS = int(ALLOWED_MEMORY-2.5)
    if torch.__version__ >= '1.8.0':
        low_ram_workflow = True
        k = ALLOWED_MEMORY/ total_memory
        torch.cuda.set_per_process_memory_fraction(k, 0)
        print('Allowed GPU RAM:', round(ALLOWED_MEMORY, 2), 'Gb')
        print('GPU part', round(k, 4))

else: # set high-ram workflow
    low_ram_workflow = False
    from rudalle.pipelines import generate_images, super_resolution
    from rudalle import get_realesrgan
    realesrgan = get_realesrgan('x2', device=device) # x2/x4/x8
    DALLE_BS = 8

# low_ram_workflow = True
if dwt_mode == "False = 256x256 high-quality":
  # vae = get_vae().to(device)  #for default 256x256 in >rc7
  if low_ram_workflow == False:
    vae = get_vae(dwt=False).to(device)  #for default 256x256 in rc7+
  else:
    vae = get_vae(dwt=False)
if dwt_mode == "True = 512x512>256x256" or dwt_mode == "True = 512x512 (artifacts)":
  if low_ram_workflow == False:
    vae = get_vae(dwt=True).to(device)   #for 512x512
  else:
    vae = get_vae(dwt=True)
ruclip, ruclip_processor = get_ruclip('ruclip-vit-base-patch32-v5')
if low_ram_workflow == False:
  ruclip = ruclip.to(device)


!pip install -U deep_translator
import time
import numpy as np
from deep_translator import GoogleTranslator, MyMemoryTranslator
# langs_dict = GoogleTranslator.get_supported_languages(as_dict=True)
# print(langs_dict)
!wget -nc https://www.1001fonts.com/download/font/open-sans.light.ttf -P /content/
from PIL import Image

def dimr(text, r=165, g=165, b=165):
    return "\033[38;2;{};{};{}m{} \033[38;2;255;255;255m".format(r, g, b, text)
def dark(text, r=115, g=115, b=115):
    return "\033[38;2;{};{};{}m{} \033[38;2;255;255;255m".format(r, g, b, text)

if low_ram_workflow == True:
  def generate_codebooks(text, tokenizer, dalle, top_k, top_p, images_num, image_prompts=None, temperature=1.0, bs=8,
                      seed=None, use_cache=True):
      vocab_size = dalle.get_param('vocab_size')
      text_seq_length = dalle.get_param('text_seq_length')
      image_seq_length = dalle.get_param('image_seq_length')
      total_seq_length = dalle.get_param('total_seq_length')
      device = dalle.get_param('device')
      text = text.lower().strip()
      input_ids = tokenizer.encode_text(text, text_seq_length=text_seq_length)
      codebooks = []
      for chunk in more_itertools.chunked(range(images_num), bs):
          chunk_bs = len(chunk)
          with torch.no_grad():
              attention_mask = torch.tril(torch.ones((chunk_bs, 1, total_seq_length, total_seq_length), device=device))
              out = input_ids.unsqueeze(0).repeat(chunk_bs, 1).to(device)
              has_cache = False
              if image_prompts is not None:
                  prompts_idx, prompts = image_prompts.image_prompts_idx, image_prompts.image_prompts
                  prompts = prompts.repeat(chunk_bs, 1)
              for idx in tqdm(range(out.shape[1], total_seq_length)):
                  idx -= text_seq_length
                  if image_prompts is not None and idx in prompts_idx:
                      out = torch.cat((out, prompts[:, idx].unsqueeze(1)), dim=-1)
                  else:
                      logits, has_cache = dalle(out, attention_mask,
                                                has_cache=has_cache, use_cache=use_cache, return_loss=False)
                      logits = logits[:, -1, vocab_size:]
                      logits /= temperature
                      filtered_logits = transformers.top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)
                      probs = torch.nn.functional.softmax(filtered_logits, dim=-1)
                      sample = torch.multinomial(probs, 1)
                      out = torch.cat((out, sample), dim=-1)
              codebooks.append(out[:, -image_seq_length:].cpu())
      return codebooks

# if you want to save to google drive, run that codeblock (near bottom) before continuing
# text = 'a sturdy red chair'
# text = 'thinking woman statue logo.'
# text = "'Calm√Ö Sailing' a popular classic oil painting of boats on the ocaean."
# text = "'Land Ahoy' a popular classic oil painting of a boat on the ocaean at sunset."
# text = 'Image of the Earth.'
# text = 'World Map'
# text = "Jungle Illustration"
# text = "'Coral Reef with Fish' - digital painting"
# text = "'Coral Reef with Irridecent Turtle' - digital painting"
# text = '–ü–∏–Ω–≥–≤–∏–Ω—ã —Ä–∞–¥—É—é—Ç—Å—è - –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ä—Ç–∏–Ω–∞ –ö–∞–∑–∏–º–∏—Ä–∞ –ú–∞–ª–µ–≤–∏—á–∞'
# text = "chrome bar trolley"  #temp1.05

text = "'Coral Reef with Fish' - digital painting"

original = text
tService = GoogleTranslator #GoogleTranslator, MyMemoryTranslator
translated = tService(source='en', target='ru').translate(text)
rev_translated = tService(source='ru', target='en').translate(translated)
print(dimr(f'original: {original}\ntranslted: {translated}\nrev-tran: {rev_translated}'))

text = translated  

